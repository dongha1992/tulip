import re
from collections import Counter

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification


# í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ì œì™¸í•  ì¡°ì‚¬Â·í”í•œ ë‹¨ì–´ (ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê¸° ìœ„í•¨)
STOPWORDS = {
    "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ë§Œ", "ì—ì„œ",
    "ìœ¼ë¡œ", "ë¡œ", "í•œ", "í•˜ë‹¤", "ìˆë‹¤", "ë˜ë‹¤", "ê·¸", "ì´", "ì €", "ê·¸ê²ƒ", "ì´ê²ƒ",
    "ì €ê²ƒ", "ë¬´ì—‡", "ì–´ë–¤", "ì–´ë””", "ì–¸ì œ", "ì™œ", "how", "the", "a", "an", "is",
    "are", "was", "were", "to", "of", "in", "on", "for", "and", "or", "but",
}


class SentimentAnalyzer:
    def __init__(self):
        # ê°ì • ì‚¬ì „ ì •ì˜
        self.EMOTION_DICT = {
            'positive': [
                'ê°ì‚¬','ë‹´ëŠ”ë‹¤','ì—…','íšŒë³µ', 'ë¶ˆì¥','ë§¤ìˆ˜', 'ğŸ‘ğŸ»', 'í˜¸ì¬', 'ì™€', 'ìœë‹¤', 'ì´ë¼',
                'ê°€ì¦ˆì•„', 'ê°€ë³´ì', 'ê°€ì', 'ê°„ë‹¤', 'ë–¡ìƒ', 'ëŒ€ë°•', 'ë¡œì¼“', 'í™”ì„±', 'ê³ ê³ ',
                'í•œê°•ë·°', 'íˆ¬ë”ë¬¸', 'ë‹¤í–‰','êµ¿', 'ì¢‹ë‹¤','ì˜¤'
            ],
            'negative': [
                'ã… ', 'ã…œ', 'ã…¡', 'ã…—', 'í•˜..', 'í•˜...', 'í..', 'í‘..', 'í—..','ì—íœ´',
                'í—ˆê±±', 'í—‰', 'í‘í‘', 'í•˜ì•„', 'í•˜..', 'ì•Šë‹¤', 'ì—†ë‹¤', 'ì•„ë‹ˆë‹¤','í•œê°•',
                'ë–¨ì–´ì§„ë‹¤', 'í­ë½', 'ì†ì‹¤', 'ì†ì ˆ', 'ë¹š', 'í•˜ë½', 'í•˜í•œê°€', 'ì¡´ë²„', 'ì¡´ë²„ì¤‘',
                'íƒˆì¶œ', 'ë¦¬ìŠ¤í¬', 'ìœ„í—˜', 'ë¶ˆì•ˆ', 'ê±±ì •', 'ë‹µì´ì—†', 'ë§í–ˆ', 'í„¸ë ¸', 'ì¡°ì •',
                'í•˜ë½ì¥', 'ì•½ì„¸ì¥','ì‚´ë ¤ì¤˜', 'ì‹¬ë€','íšŒì˜ì ', 'ê³¡ì†Œë¦¬', 'ë¹šíˆ¬', 'ì†ì‹¤',
                'ì‚´ë ¤ì¤˜', 'ì†í•´','ì¡ì£¼','ë§í–ˆë‹¤', 'í­ë§', 'ëŒ€í­ë½', 'ê³µí¬','ìŠ¤íŠ¸ë ˆìŠ¤',
                'ë–¨ì–´ì§€ë©´','ëŸ´', 'íŒ¨ë‹‰ì…€','ê°œë¬´ì„­ë„¤','ì¡°ì§„','ê³ ì ì‹ í˜¸','ì£½ì‘¤ê³ ','ì„¤ê±°ì§€','ìŒ','í '
            ],
            'very_negative': [
                'ìœ¼ì•„', 'ì‚´ë ¤', 'í•˜..', 'í•˜...', 'í‘..', 'í—..', 'ì—íœ´','í•œê°•ê°€ì',
                'í•œê°•ë¬¼', 'ì–´íœ´', 'ìƒí', 'ì‚­ì œ','ã…¡ã…¡','ìˆ¨ê¹€','ì•ˆë¼','ì•ˆë¼ìš”','ì•ˆë¼',
            ],
            'swear_words': [
                'ã……ã…‚', 'ã…‚ã……', 'ã…ˆã„¹', 'ã…ã…Š', 'ã……ã„²','ã…ˆã„´', 'ã…—ã…—',
                'ê°œã……ã„²', 'ê°œã…‚ã……', 'ê°œã…ˆã„¹', 'ê°œã……ã…‚','ê°œã…ã…Š', 'ë ¨',
                'ì§€ã„¹', 'ë‹¥ã…Š', 'ã……ã„²ë“¤', 'ã…‚ã……ë“¤', 'ã…ˆã„¹í•˜ë„¤',
                'ë¯¸ã…Š', 'ë¹¡ì¹˜', 'ê°œìƒˆ', 'ì‹­', 'ì‹œã…‚', 'ë³‘ã……'
            ]
        }

        self.LABEL_MAPPING = {
            0: "ë¶€ì •",
            1: "ë‹¤ì†Œ ë¶€ì •",
            2: "ì¤‘ë¦½",
            3: "ë‹¤ì†Œ ê¸ì •",
            4: "ê¸ì •"
        }

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base", local_files_only=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "beomi/kcbert-base",
            num_labels=5,
            local_files_only=True
        )

    def preprocess_text(self, text):
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def extract_emotion_features(self, text):
        text = self.preprocess_text(text)
        has_positive = any(em in text for em in self.EMOTION_DICT['positive'])
        has_negative = any(em in text for em in self.EMOTION_DICT['negative'])
        has_very_negative = any(em in text for em in self.EMOTION_DICT['very_negative'])
        has_swear = any(em in text for em in self.EMOTION_DICT['swear_words'])
        return has_positive, has_negative, has_very_negative, has_swear

    def analyze_text(self, text):
        try:
            emotion_features = self.extract_emotion_features(text)
            has_positive, has_negative, has_very_negative, has_swear = emotion_features

            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=300,
                padding="max_length"
            )

            with torch.no_grad():
                outputs = self.model(**inputs)

            probabilities = F.softmax(outputs.logits, dim=-1)
            scores = probabilities[0].tolist()
            base_score = sum([score * (i * 25) for i, score in enumerate(scores)])

            # ì ìˆ˜ ì¡°ì •
            if has_swear:
                base_score = 0
            elif has_positive:
                base_score = min(100, base_score + 40)
                if '!!' in text or '!!!!' in text:
                    base_score = max(90, base_score)
                if 'ğŸ‘ğŸ»' in text:
                    base_score = min(100, base_score + 10)

            if has_very_negative:
                base_score = max(0, base_score - 60)
            elif has_negative:
                base_score = max(0, base_score - 40)

            # ê°ì • ë¶„ë¥˜
            if base_score >= 75:
                sentiment = "ê¸ì •"
            elif base_score >= 55:
                sentiment = "ë‹¤ì†Œ ê¸ì •"
            elif base_score > 45 and base_score < 55:
                sentiment = "ì¤‘ë¦½"
            elif base_score >= 25:
                sentiment = "ë‹¤ì†Œ ë¶€ì •"
            else:
                sentiment = "ë¶€ì •"

            return {
                'score': base_score,
                'label': sentiment,
                'confidence': "ë†’ìŒ" if abs(base_score - 50) > 20 else "ì¤‘ê°„",
                'class_probabilities': {
                    self.LABEL_MAPPING[i]: round(score * 100, 2)
                    for i, score in enumerate(scores)
                }
            }

        except Exception as e:
            print(f"Error processing text: {str(e)}")
            return {
                'score': 50.0,
                'label': "ì¤‘ë¦½",
                'confidence': "ë‚®ìŒ"
            }

    def extract_top_keywords(self, texts: list[str], top_n: int = 3) -> list[str]:
        """
        í”¼ë“œ í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ë§ì´ ë“±ì¥í•œ, ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ìƒìœ„ top_nê°œ ë°˜í™˜.
        ì¡°ì‚¬Â·ê°ì •ì‚¬ì „Â·stopword ì œì™¸ í›„ ë¹ˆë„ ê¸°ì¤€.
        """
        if not texts:
            return []

        # ê°ì • ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” í‚¤ì›Œë“œ í›„ë³´ì—ì„œ ì œì™¸ (ì˜ë¯¸ ìˆëŠ” ì£¼ì œì–´ ìœ„ì£¼)
        exclude = set(STOPWORDS)
        for key in ("positive", "negative", "very_negative", "swear_words"):
            exclude.update(self.EMOTION_DICT[key])

        # í•œê¸€Â·ì˜ë¬¸Â·ìˆ«ì ì—°ì†ë§Œ í† í°ìœ¼ë¡œ (2ì ì´ìƒ)
        token_re = re.compile(r"[ê°€-í£a-zA-Z0-9]{2,}")
        counter: Counter[str] = Counter()

        for text in texts:
            if not text or not isinstance(text, str):
                continue
            normalized = self.preprocess_text(text)
            tokens = token_re.findall(normalized)
            for t in tokens:
                t_lower = t.lower()
                if t_lower in exclude or t in exclude:
                    continue
                # ìˆ«ìë§Œ ìˆëŠ” í† í° ì œì™¸
                if t.isdigit():
                    continue
                counter[t_lower] += 1

        # ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ, ë™ì ì´ë©´ ì›ë¬¸ ë“±ì¥ ìˆœì„œ ìœ ì§€í•˜ê³  ì‹¶ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ê³  ìƒìœ„ nê°œ
        top = counter.most_common(top_n)
        return [word for word, _ in top]